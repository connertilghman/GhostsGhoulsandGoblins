y = y_train_factor,
method = "svmRadial",
tuneGrid = tune_grid,
trControl = train_control,
preProcess = c("center", "scale")
)
library(dplyr)
# For data visualizations
library(ggplot2)
library(fpc)
install.packages("fpc")
library(fpc)
# For modeling and predictions
library(caret)
library(glmnet)
library(ranger)
library(e1071)
library(clValid)
install.packages("clValid")
library(clValid)
train <- read.csv('../input/train.csv', header = TRUE, stringsAsFactors = FALSE)
train$Dataset <- "train"
train <- read.csv('train.csv')
train$Dataset <- "train"
test <- read.csv('../input/test.csv', header = TRUE, stringsAsFactors = FALSE)
test$Dataset <- "test"
test <- read.csv('test.csv')
test$Dataset <- "test"
full <- bind_rows(train, test)
str(full)
summary(full)
factor_variables <- c('id', 'color', 'type', 'Dataset')
full[factor_variables] <- lapply(full[factor_variables], function(x) as.factor(x))
train_2 <- full[full$Dataset == 'train', ]
ggplot(train_2,
aes(x = type,
y = bone_length,
fill = type)) +
geom_boxplot() +
guides(fill = FALSE) +
xlab("Creature") +
ylab("Bone Length") +
scale_fill_manual(values = c("#D55E00", "#0072B2", "#009E73"))
ggplot(train_2,
aes(x = type,
y = rotting_flesh,
fill = type)) +
geom_boxplot() +
guides(fill = FALSE) +
xlab("Creature") +
ylab("Percentage of Rotting Flesh") +
scale_fill_manual(values = c("#D55E00", "#0072B2", "#009E73"))
ggplot(train_2,
aes(x = type,
y = hair_length,
fill = type)) +
geom_boxplot() +
guides(fill = FALSE) +
xlab("Creature") +
ylab("Hair Length") +
scale_fill_manual(values = c("#D55E00", "#0072B2", "#009E73"))
ggplot(train_2,
aes(x = type,
y = has_soul,
fill = type)) +
geom_boxplot() +
guides(fill = FALSE) +
xlab("Creature") +
ylab("Percentage of Soul Present") +
scale_fill_manual(values = c("#D55E00", "#0072B2", "#009E73"))
pairs(full[,2:5],
col = full$type,
labels = c("Bone Length", "Rotting Flesh", "Hair Length", "Soul"))
full <- full %>%
mutate(hair_soul = hair_length * has_soul)
full_1 <- full %>%
filter(!is.na(type))
full <- full %>%
mutate(bone_flesh = bone_length * rotting_flesh,
bone_hair = bone_length * hair_length,
bone_soul = bone_length * has_soul,
flesh_hair = rotting_flesh * hair_length,
flesh_soul = rotting_flesh * has_soul)
summary(full)
# Extract creature labels and remove column from dataset
creature_labels <- full$type
full2 <- full
full2$type <- NULL
# Remove categorical variables (id, color, and dataset) from dataset
full2$id <- NULL
full2$color <- NULL
full2$Dataset <- NULL
# Perform k-means clustering with 3 clusters, repeat 30 times
creature_km_1 <- kmeans(full2, 3, nstart = 30)
plotcluster(full2, creature_km_1$cluster)
dunn_ckm_1 <- dunn(clusters = creature_km_1$cluster, Data = full2)
# Print results
dunn_ckm_1
table(creature_km_1$cluster, creature_labels)
train_complete <- full[full$Dataset == 'train', ]
test_complete <- full[full$Dataset == 'test', ]
myControl <- trainControl(
method = "cv",
number = 10,
repeats = 20,
verboseIter = TRUE
)
rf_model <- train(
type ~ bone_length + rotting_flesh + hair_length + has_soul + color + hair_soul + bone_flesh + bone_hair +
bone_soul + flesh_hair + flesh_soul,
tuneLength = 3,
data = train_complete,
method = "ranger",
trControl = myControl,
importance = 'impurity'
)
# Creating a Variable Importance variable
vimp <- varImp(rf_model)
rf_model_2 <- train(
type ~ bone_length + rotting_flesh + hair_length + has_soul + hair_soul + bone_flesh + bone_hair +
bone_soul + flesh_hair + flesh_soul,
tuneLength = 3,
data = train_complete,
method = "ranger",
trControl = myControl,
importance = 'impurity'
)
glm_model <- train(
type ~ bone_length + rotting_flesh + hair_length + has_soul + color + hair_soul + bone_flesh + bone_hair +
bone_soul + flesh_hair + flesh_soul,
method = "glmnet",
tuneGrid = expand.grid(alpha = 0:1,
lambda = seq(0.0001, 1, length = 20)),
data = train_complete,
trControl = myControl
)
glm_model_2 <- train(
type ~ bone_length + rotting_flesh + hair_length + has_soul + hair_soul + bone_flesh + bone_hair +
bone_soul + flesh_hair + flesh_soul,
method = "glmnet",
tuneGrid = expand.grid(alpha = 0:1,
lambda = seq(0.0001, 1, length = 20)),
data = train_complete,
trControl = myControl
)
# Create a list of models
models <- list(rf = rf_model, rf2 = rf_model_2, glmnet = glm_model, glmnet2 = glm_model_2)
# Resample the models
resampled <- resamples(models)
# Generate a summary
summary(resampled)
# Plot the differences between model fits
dotplot(resampled, metric = "Accuracy")
# Reorder the data by creature ID number
test_complete <- test_complete %>%
arrange(id)
# Make predicted survival values
my_prediction <- predict(glm_model_2, test_complete)
# Create a data frame with two columns
my_submission <- data.frame(id = test_complete$id, Type = my_prediction)
# Write the solution to a csv file
write.csv(my_submission, file = "my_submission.csv", row.names = FALSE)
y_train <- model.matrix(~ type - 1, data=train)
train$type <- NULL
# Combine train/test for preprocessing
train_test <- bind_rows(train, test)
# Drop color column (assume it's not useful)
train_test$color <- NULL
# Split back
X_train <- as.matrix(train_test[1:nrow(y_train), ])
X_test  <- as.matrix(train_test[(nrow(y_train)+1):nrow(train_test), ])
# Sigmoid and softmax functions
sigmoid <- function(x) {
1 / (1 + exp(-x))
}
softmax <- function(x) {
exp_x <- exp(x)
exp_x / sum(exp_x)
}
# Simple neural network class
neuralnet <- function(num_input, num_hidden, num_output) {
list(
W1 = matrix(runif(num_input * num_hidden, -0.5, 0.5), nrow=num_input, ncol=num_hidden),
b1 = matrix(0, nrow=1, ncol=num_hidden),
W2 = matrix(runif(num_hidden * num_output, -0.5, 0.5), nrow=num_hidden, ncol=num_output),
b2 = matrix(0, nrow=1, ncol=num_output)
)
}
forward <- function(nn, X) {
net1 <- X %*% nn$W1 + nn$b1
y <- sigmoid(net1)
net2 <- y %*% nn$W2 + nn$b2
z <- softmax(net2)
list(z=z, y=y)
}
backpropagation <- function(nn, X, target, eta) {
fwd <- forward(nn, X)
z <- fwd$z
y <- fwd$y
d2 <- (z - target)
d1 <- y * (1 - y) * (d2 %*% t(nn$W2))
nn$W2 <- nn$W2 - t(y) %*% d2 * eta
nn$W1 <- nn$W1 - t(matrix(X, ncol=1)) %*% d1 * eta
nn$b2 <- nn$b2 - d2 * eta
nn$b1 <- nn$b1 - d1 * eta
nn
}
# Hyperparameters
num_hidden <- 12
n_epochs <- 2000
eta <- 0.01
# Initialize network
nn <- neuralnet(ncol(X_train), num_hidden, ncol(y_train))
# Training
for(epoch in 1:n_epochs) {
for(i in 1:nrow(X_train)) {
nn <- backpropagation(nn, X_train[i, , drop=FALSE], y_train[i, , drop=FALSE], eta)
}
}
# Predictions
preds <- apply(X_test, 1, function(monster) {
z <- forward(nn, matrix(monster, nrow=1))$z
colnames(y_train)[which.max(z)]
})
for(i in 1:nrow(X_train)) {
nn <- backpropagation(nn, X_train[i, , drop=FALSE], y_train[i, , drop=FALSE], eta)
}
backpropagation <- function(nn, X, target, eta) {
fwd <- forward(nn, X)
z <- fwd$z
y <- fwd$y
d2 <- (z - target)
d1 <- y * (1 - y) * (d2 %*% t(nn$W2))
nn$W2 <- nn$W2 - t(y) %*% d2 * eta
nn$W1 <- nn$W1 - t(matrix(X, ncol=1)) %*% d1 * eta
nn$b2 <- nn$b2 - d2 * eta
nn$b1 <- nn$b1 - d1 * eta
nn
}
# Training
for(epoch in 1:n_epochs) {
for(i in 1:nrow(X_train)) {
nn <- backpropagation(nn, X_train[i, , drop=FALSE], y_train[i, , drop=FALSE], eta)
}
}
backpropagation <- function(nn, X, target, eta) {
fwd <- forward(nn, X)
z <- fwd$z
y <- fwd$y
d2 <- (z - target)
d1 <- y * (1 - y) * (d2 %*% t(nn$W2))
nn$W2 <- nn$W2 - t(y) %*% d2 * eta
nn$W1 <- nn$W1 - t(matrix(X, ncol=1)) %*% d1 * eta
nn$b2 <- nn$b2 - d2 * eta
nn$b1 <- nn$b1 - d1 * eta
nn
}
# Predictions
preds <- apply(X_test, 1, function(monster) {
z <- forward(nn, matrix(monster, nrow=1))$z
colnames(y_train)[which.max(z)]
})
preds <- apply(X_test, 1, function(monster) {
z <- forward(nn, matrix(monster, nrow=1))$z
colnames(y_train)[which.max(z)]
})
# Save submission
submission <- data.frame(id=rownames(X_test), type=preds)
library(tidymodels)
library(keras)
install.packages("keras")
library(keras)
# Load data
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Convert target to factor
train$type <- as.factor(train$type)
# Recipe: one-hot encode categorical predictors and remove zero variance
nn_recipe <- recipe(type ~ ., data = train) |>
update_role(id, new_role = "ID") |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors())
# Load data
train <- read_csv("train.csv")
library(tidymodels)
library(keras)
library(readr)
library(dplyr)
# Load data
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Convert target to factor
train$type <- as.factor(train$type)
# Recipe: one-hot encode categorical predictors and remove zero variance
nn_recipe <- recipe(type ~ ., data = train) |>
update_role(id, new_role = "ID") |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors())
# Prepare training data
nn_prep <- prep(nn_recipe)
X_train <- bake(nn_prep, new_data = NULL) |> select(-type)
y_train <- bake(nn_prep, new_data = NULL) |> pull(type) |> as.integer() - 1  # keras uses 0-index
num_classes <- length(levels(train$type))
X_test <- bake(nn_prep, new_data = test) |> select(-id)
# Define Keras neural network model
nn_model <- keras_model_sequential() %>%
layer_dense(units = 12, activation = "sigmoid", input_shape = ncol(X_train)) %>%
layer_dense(units = num_classes, activation = "softmax")
# Compile model
nn_model %>% compile(
optimizer = optimizer_sgd(learning_rate = 0.01),
loss = "sparse_categorical_crossentropy",
metrics = "accuracy"
)
library(tidymodels)
library(readr)
library(dplyr)
# Load data
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Convert target to factor
train$type <- as.factor(train$type)
# Recipe: dummy encode categorical predictors and remove zero-variance predictors
nn_recipe <- recipe(type ~ ., data = train) |>
update_role(id, new_role = "ID") |>
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors())
# Define neural network model
nn_model <- mlp(
hidden_units = tune(),    # number of neurons in hidden layer
penalty = tune(),         # L2 regularization
epochs = 200              # number of epochs
) |>
set_engine("nnet") |>
set_mode("classification")
# Workflow
nn_wf <- workflow() |>
add_recipe(nn_recipe) |>
add_model(nn_model)
# Cross-validation folds
cv_folds <- vfold_cv(train, v = 10, repeats = 5, strata = type)
# Hyperparameter grid
nn_grid <- grid_regular(
hidden_units(range = c(5, 20)),
penalty(range = c(0, 0.1)),
levels = 5
)
# Tune model
nn_tuned <- tune_grid(
nn_wf,
resamples = cv_folds,
grid = nn_grid,
metrics = metric_set(accuracy)
)
# Select best parameters
best_params <- select_best(nn_tuned, metric = "accuracy")
# Finalize workflow with best parameters
final_nn <- finalize_workflow(nn_wf, best_params)
# Fit final model on full training data
final_fit <- fit(final_nn, data = train)
# Predict on test set
test_preds <- predict(final_fit, test) |>
bind_cols(test |> select(id))
# Prepare submission
submission <- test_preds |>
select(id, .pred_class) |>
rename(type = .pred_class)
# Save submission
write_csv(submission, "submission.csv")
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Convert target to integer factor for torch
train$type <- as.factor(train$type)
y_train <- as.integer(train$type) - 1  # 0-indexed
# Prepare predictors
X_train <- train |> select(-id, -type) |> as.matrix() |> torch_tensor(dtype = torch_float())
X_test  <- test  |> select(-id) |> as.matrix() |> torch_tensor(dtype = torch_float())
library(torch)
library(readr)
library(dplyr)
install.packages("torch")
train <- read_csv("train.csv")
library(torch)
library(readr)
library(dplyr)
# Convert target to integer factor for torch
train$type <- as.factor(train$type)
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Convert target to integer factor for torch
train$type <- as.factor(train$type)
y_train <- as.integer(train$type) - 1  # 0-indexed
# Prepare predictors
X_train <- train |> select(-id, -type) |> as.matrix() |> torch_tensor(dtype = torch_float())
# Convert target to integer factor for torch
train$type <- as.factor(train$type)
y_train <- as.integer(train$type) - 1  # 0-indexed
# Prepare predictors
X_train <- train |> select(-id, -type) |> as.matrix() |> torch_tensor(dtype = torch_float())
library(fastDummies) # for one-hot encoding
install.packages("fastDummies")
# Load data
train <- read_csv("train.csv")
test  <- read_csv("test.csv")
# Convert target to integer factor for torch
train$type <- as.factor(train$type)
y_train <- as.integer(train$type) - 1  # 0-indexed
# Combine train/test to ensure consistent dummy encoding
train_test <- bind_rows(
train |> select(-type),
test
)
# One-hot encode all categorical columns
train_test <- fastDummies::dummy_cols(train_test, remove_selected_columns = TRUE)
# Split back into train/test
X_train_mat <- as.matrix(train_test[1:nrow(train), ])
X_test_mat  <- as.matrix(train_test[(nrow(train)+1):nrow(train_test), ])
# Convert to torch tensors
X_train <- torch_tensor(X_train_mat, dtype = torch_float())
X_test  <- torch_tensor(X_test_mat, dtype = torch_float())
# Number of classes
num_classes <- length(levels(train$type))
# Define neural network
net <- nn_module(
initialize = function(input_size, hidden_size, output_size) {
self$fc1 <- nn_linear(input_size, hidden_size)
self$fc2 <- nn_linear(hidden_size, output_size)
},
forward = function(x) {
x %>% self$fc1() %>% nnf_sigmoid() %>% self$fc2() %>% nnf_softmax(dim = 2)
}
)
# Hyperparameters
input_size  <- ncol(X_train)
hidden_size <- 12
output_size <- num_classes
lr <- 0.01
epochs <- 2000
# Instantiate model
model <- net(input_size, hidden_size, output_size)
# Optimizer and loss
optimizer <- optim_sgd(model$parameters, lr = lr)
loss_fn <- nn_cross_entropy_loss()
# Training loop
for(epoch in 1:epochs) {
optimizer$zero_grad()
output <- model(X_train)
loss <- loss_fn(output, torch_tensor(y_train, dtype = torch_long()))
loss$backward()
optimizer$step()
if(epoch %% 100 == 0) {
cat("Epoch:", epoch, "Loss:", loss$item(), "\n")
}
}
y_train <- as.integer(train$type) # 0-indexed
# Combine train/test to ensure consistent dummy encoding
train_test <- bind_rows(
train |> select(-type),
test
)
# One-hot encode all categorical columns
train_test <- fastDummies::dummy_cols(train_test, remove_selected_columns = TRUE)
# Split back into train/test
X_train_mat <- as.matrix(train_test[1:nrow(train), ])
X_test_mat  <- as.matrix(train_test[(nrow(train)+1):nrow(train_test), ])
# Convert to torch tensors
X_train <- torch_tensor(X_train_mat, dtype = torch_float())
X_test  <- torch_tensor(X_test_mat, dtype = torch_float())
# Number of classes
num_classes <- length(levels(train$type))
# Define neural network
net <- nn_module(
initialize = function(input_size, hidden_size, output_size) {
self$fc1 <- nn_linear(input_size, hidden_size)
self$fc2 <- nn_linear(hidden_size, output_size)
},
forward = function(x) {
x %>% self$fc1() %>% nnf_sigmoid() %>% self$fc2() %>% nnf_softmax(dim = 2)
}
)
# Hyperparameters
input_size  <- ncol(X_train)
hidden_size <- 12
output_size <- num_classes
lr <- 0.01
epochs <- 2000
# Instantiate model
model <- net(input_size, hidden_size, output_size)
# Optimizer and loss
optimizer <- optim_sgd(model$parameters, lr = lr)
loss_fn <- nn_cross_entropy_loss()
# Training loop
for(epoch in 1:epochs) {
optimizer$zero_grad()
output <- model(X_train)
loss <- loss_fn(output, torch_tensor(y_train, dtype = torch_long()))
loss$backward()
optimizer$step()
if(epoch %% 100 == 0) {
cat("Epoch:", epoch, "Loss:", loss$item(), "\n")
}
}
# Predictions
pred_probs <- model(X_test)
pred_classes <- pred_probs$argmax(dim = 2)$to(dtype = torch_int())$cpu()$numpy()
pred_classes <- pred_probs %>%
torch_argmax(dim = 2) %>%   # pick max along class dimension
as_array()
pred_labels <- levels(train$type)[pred_classes + 1]
# Submission
submission <- test |> select(id) |> mutate(type = pred_labels)
write_csv(submission, "submission.csv")
